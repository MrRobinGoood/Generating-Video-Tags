{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "212d7aaa-0d46-4b17-b77d-cb015a705167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1)\n",
      "Requirement already satisfied: torchvision in ./.local/lib/python3.10/site-packages (0.19.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.45.1)\n",
      "Requirement already satisfied: timm in ./.local/lib/python3.10/site-packages (1.0.9)\n",
      "Requirement already satisfied: einops in ./.local/lib/python3.10/site-packages (0.8.0)\n",
      "Requirement already satisfied: opencv-python in ./.local/lib/python3.10/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.10/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch) (3.0.3)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.68)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.local/lib/python3.10/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#для VMAE, BLIP, BART\n",
    "%pip install torch torchvision transformers timm einops opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b30b64-5c8b-49be-8518-3780dfe359cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#для WHISPER\n",
    "!sudo apt install ffmpeg -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa65af6a-1c62-43a1-b360-ebfe162ff057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.45.1)\n",
      "Requirement already satisfied: accelerate in ./.local/lib/python3.10/site-packages (0.34.2)\n",
      "Requirement already satisfied: ffmpeg-python in ./.local/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.4.1)\n",
      "Requirement already satisfied: future in ./.local/lib/python3.10/site-packages (from ffmpeg-python) (1.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch>=1.10.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.0.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.6.68)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#для WHISPER\n",
    "%pip install --upgrade transformers accelerate ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9de4327b-939c-47c7-b4c5-d43fb02e9889",
   "metadata": {},
   "outputs": [],
   "source": [
    "#для VMAE, BLIP, BART\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import cv2  # OpenCV для обработки видео\n",
    "import os\n",
    "from transformers import VideoMAEModel, VideoMAEFeatureExtractor\n",
    "from transformers import BlipForConditionalGeneration, BlipProcessor\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Определение устройства\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99f26500-bae7-4fc0-abdf-6cea8cd9c107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "#для WHISPER\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import ffmpeg\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "device2 = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device2}\")\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9221b6d9-e03b-4412-9821-2290a797b838",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"resources/source_video\"\n",
    "audio_path = \"resources/converted_audio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b211dc5-3c14-4d5b-a17a-323774d5295c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#методы VMAE, BLIP, BART\n",
    "def load_video_frames_from_mp4(path, max_frames=16, resize=(224, 224)):\n",
    "    \"\"\"\n",
    "    Извлечение кадров из видеофайла и их предобработка.\n",
    "\n",
    "    Args:\n",
    "        path (str): Путь к видеофайлу (.mp4).\n",
    "        max_frames (int): Максимальное количество кадров для обработки.\n",
    "        resize (tuple): Размер для изменения размера кадров.\n",
    "\n",
    "    Returns: torch.Tensor: Тензор вида [1, num_frames, 3, height, width]\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Не удалось открыть видеофайл: {path}\")\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f\"Общее количество кадров в видео: {total_frames}\")\n",
    "\n",
    "    frame_idx = 0\n",
    "    while len(frames) < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Достигнут конец видео или произошла ошибка при чтении кадра.\")\n",
    "            break\n",
    "\n",
    "        # Конвертация из BGR (OpenCV) в RGB (PIL)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(frame)\n",
    "\n",
    "        transform = T.Compose([\n",
    "            T.Resize(resize),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.5, 0.5, 0.5],  # Средние значения нормализации\n",
    "                        std=[0.5, 0.5, 0.5])   # Стандартные отклонения нормализации\n",
    "        ])\n",
    "        image = transform(image)\n",
    "        frames.append(image)\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if len(frames) == 0:\n",
    "        raise ValueError(\"Не удалось извлечь ни одного кадра из видео.\")\n",
    "\n",
    "    # Если кадров меньше, чем max_frames, дублируем последние кадры\n",
    "    while len(frames) < max_frames:\n",
    "        frames.append(frames[-1].clone())\n",
    "\n",
    "    video_tensor = torch.stack(frames)  # [num_frames, 3, height, width]\n",
    "    video_tensor = video_tensor.unsqueeze(0)  # [1, num_frames, 3, height, width]\n",
    "    return video_tensor\n",
    "\n",
    "def extract_features_with_videomae(pixel_values, device):\n",
    "    \"\"\"\n",
    "    Извлечение признаков из видео с использованием предобученной модели VideoMAE.\n",
    "\n",
    "    Args:\n",
    "        pixel_values (torch.Tensor): Тензор формы [batch_size, num_frames, 3, height, width]\n",
    "        device (torch.device): Устройство для вычислений (GPU или CPU)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Извлеченные признаки.\n",
    "    \"\"\"\n",
    "    # Загрузка предобученного извлекателя признаков и модели\n",
    "    feature_extractor = VideoMAEFeatureExtractor.from_pretrained(\"MCG-NJU/videomae-large\")\n",
    "    model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-large\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Перемещение данных на устройство\n",
    "    pixel_values = pixel_values.to(device)\n",
    "\n",
    "    # Проверка формы входных данных\n",
    "    if pixel_values.ndim != 5:\n",
    "        raise ValueError(f\"Ожидается 5 измерений, получено {pixel_values.ndim}\")\n",
    "\n",
    "    batch_size, num_frames, num_channels, height, width = pixel_values.shape\n",
    "    assert num_channels == 3, \"Ожидается 3 канала (RGB)\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "        # Получаем последнее скрытое состояние\n",
    "        features = outputs.last_hidden_state  # Вид: [batch_size, num_frames, hidden_size]\n",
    "    return features\n",
    "\n",
    "def generate_description_with_blip(pixel_values, device):\n",
    "    \"\"\"\n",
    "    Генерация описания к видео с использованием модели BLIP.\n",
    "\n",
    "    Args:\n",
    "        pixel_values (torch.Tensor): Тензор формы [batch_size, num_frames, 3, height, width]\n",
    "        device (torch.device): Устройство для вычислений (GPU или CPU)\n",
    "\n",
    "    Returns:\n",
    "        str: Сгенерированное описание.\n",
    "    \"\"\"\n",
    "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    batch_size, num_frames, num_channels, height, width = pixel_values.shape\n",
    "    descriptions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_frames):\n",
    "            frame = pixel_values[:, i, :, :, :]  # [batch_size, 3, height, width]\n",
    "            # Перемещение кадра на устройство\n",
    "            frame = frame.to(device)\n",
    "\n",
    "            # Преобразуем тензор в список изображений\n",
    "            images = frame.cpu().numpy()\n",
    "            images = [Image.fromarray(np.uint8(np.clip(img.transpose(1, 2, 0) * 255, 0, 255))) for img in images]\n",
    "\n",
    "            inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "            outputs = model.generate(**inputs)\n",
    "            captions = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "            descriptions.extend(captions)\n",
    "\n",
    "    # Объединяем описания отдельных кадров\n",
    "    final_description = \" \".join(descriptions)\n",
    "    return final_description\n",
    "\n",
    "def summarize_text(text, device, model_name=\"facebook/bart-large-cnn\", max_length=150, min_length=40):\n",
    "    \"\"\"\n",
    "    Суммаризация текста с использованием предобученной модели.\n",
    "\n",
    "    Args:\n",
    "        text (str): Текст для суммаризации.\n",
    "        device (torch.device): Устройство для вычислений (GPU или CPU).\n",
    "        model_name (str): Название предобученной модели суммаризации.\n",
    "        max_length (int): Максимальная длина суммаризованного текста.\n",
    "        min_length (int): Минимальная длина суммаризованного текста.\n",
    "\n",
    "    Returns:\n",
    "        str: Суммаризованный текст.\n",
    "    \"\"\"\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(inputs, max_length=max_length, min_length=min_length, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "def get_text_from_video(video_id):\n",
    "    path = f'{video_path}/{video_id}.mp4' \n",
    "     # Шаг 1: Извлечение кадров из видео\n",
    "    try:\n",
    "        pixel_values = load_video_frames_from_mp4(path)\n",
    "        print(f\"Форма pixel_values: {pixel_values.shape}\")  # Ожидается [1, num_frames, 3, 224, 224]\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке кадров: {e}\")\n",
    "        return\n",
    "\n",
    "    # Шаг 2: Извлечение признаков с помощью VideoMAE\n",
    "    try:\n",
    "        features = extract_features_with_videomae(pixel_values, device)\n",
    "        print(f\"Форма извлеченных признаков: {features.shape}\")  # Пример: [1, num_frames, hidden_size]\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при извлечении признаков с VideoMAE: {e}\")\n",
    "        return\n",
    "\n",
    "    # Шаг 3: Генерация описания с помощью BLIP\n",
    "    try:\n",
    "        description = generate_description_with_blip(pixel_values, device)\n",
    "        print(description)\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при генерации описания с BLIP: {e}\")\n",
    "        return\n",
    "\n",
    "    # Шаг 4: Суммаризация полученного описания\n",
    "    try:\n",
    "        summary = summarize_text(description, device)\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при суммаризации описания: {e}\")\n",
    "        return   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67e7e620-aef9-43d0-aa44-8121b8d61c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#whisper model\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device2)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device2,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1f2c4f5-3e2b-4ea3-881a-9ba0679607fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#метод конвертации mp4 в mp3 для whisper\n",
    "def convert_video_to_audio(input_path, output_path):\n",
    "    try:\n",
    "        # Проверяем существование входного файла\n",
    "        input_file = Path(input_path)\n",
    "        if not input_file.exists():\n",
    "            raise FileNotFoundError(f\"Файл {input_path} не найден\")\n",
    "\n",
    "        # Выполняем преобразование\n",
    "        (\n",
    "            ffmpeg.input(input_path)\n",
    "            .output(output_path, format='mp3')\n",
    "            .overwrite_output()\n",
    "            .run(capture_stdout=True, capture_stderr=True)\n",
    "        )\n",
    "\n",
    "        # print(f\"Преобразование завершено успешно. Результат сохранен как {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Произошла ошибка при преобразовании: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d108502-ce9a-412c-b0ce-7bd184c4e329",
   "metadata": {},
   "outputs": [],
   "source": [
    "#whisper конвертация и расшифровка аудио с сумаризацией\n",
    "def get_text_from_audio(video_id):\n",
    "    convert_video_to_audio(f'{video_path}/{video_id}.mp4', f'{audio_path}/{video_id}.mp3')\n",
    "    text = pipe(f'{audio_path}/{video_id}.mp3', return_timestamps=True)['text']\n",
    "    return  summarize_text(text, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4aed038f-7ee5-4179-8f72-4efce3fcd135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общее количество кадров в видео: 486\n",
      "Форма pixel_values: torch.Size([1, 16, 3, 224, 224])\n",
      "Форма извлеченных признаков: torch.Size([1, 1568, 1024])\n",
      "a close up of a computer screen with a purple background a close up of a computer screen with a purple background a close up of a computer screen with a purple background a close up of a computer screen with a purple background a close up of a computer screen with a purple background a close up of a computer screen with a purple background a close up of a computer screen with a purple background a close up of a computer screen with a purple background a close up of a computer screen with a purple background a close up of a computer screen with a purple background a close up of a computer screen with a purple background a close up of a computer screen with a purple background a close up of a computer screen with a purple background a close up of a computer screen with a purple background a close up of a computer screen with a purple background a close up of a computer screen with a purple background\n",
      "Close up of a computer screen with a purple background. Close up of computer screens with purple backgrounds. Close-ups of computers with a black and white background. A close-up of a purple computer screen and a computer with a blue background.\n"
     ]
    }
   ],
   "source": [
    "print(get_text_from_video('1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9088a633-c825-4226-851c-c9de5c077d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarize:  Раз, два, три, четыре, пять. Сейчас  работает  фигнёй,    “оже”,  “Не  такте,”  ‘’’, “‘‘,’ ’” ‘, ‘,'’  ’, ’.\n"
     ]
    }
   ],
   "source": [
    "print(get_text_from_audio('1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56b7c29-787e-4779-b9af-5b1d1b88e0e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
